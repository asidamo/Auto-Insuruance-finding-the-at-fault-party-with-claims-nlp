{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "plt.rcParams['figure.figsize'] = (12,10)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Import  scikit modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "# NLP modules\n",
    "import nltk \n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "import spacy\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en='en_core_web_sm'\n",
    "nlp=spacy.load(en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_df=pd.read_csv('data/NSS_DS_data.thegeneral.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# severity of accidents \n",
    "claimgroup_df=general_df[['ClaimID_RGEN','ClaimGroup']].groupby('ClaimGroup').count()\n",
    "# total claim\n",
    "total_claim=claimgroup_df['ClaimID_RGEN'].sum()\n",
    "# change it to percentage share by claimgroup\n",
    "claimgroup_df['ClaimID_RGEN']=claimgroup_df['ClaimID_RGEN']/total_claim\n",
    "\n",
    "# rename columns and then plot\n",
    "claimgroup_df.rename(columns={'ClaimID_RGEN':'count'},inplace=True)\n",
    "claimgroup_df.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# severity of accidents \n",
    "severity_df=general_df[['ClaimID_RGEN','SeverityTypeName']].groupby('SeverityTypeName').count()\n",
    "# rename columns and then plot\n",
    "severity_df.rename(columns={'ClaimID_RGEN':'count'},inplace=True)\n",
    "severity_df.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# severity of accidents \n",
    "severity_byState=general_df[['StateName','SeverityTypeName','ClaimID_RGEN']].groupby(['StateName','SeverityTypeName']).count()\n",
    "severity_byState.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns and then plot\n",
    "severity_byState.rename(columns={'ClaimID_RGEN':'count'},inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "severity_byState.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "severity_byState=severity_byState.reset_index('SeverityTypeName')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "severity_byState.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "severity_byState=severity_byState.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "severity_byState.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# major accidents by state\n",
    "severity_byState_major=severity_byState[severity_byState.SeverityTypeName=='Major (hospitalization 3+ days or ICU)']\n",
    "severity_byState_major=severity_byState_major.drop('SeverityTypeName',axis=1)\n",
    "severity_byState_major=severity_byState_major.set_index('StateName')\n",
    "severity_byState_major.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moderate accidents by state\n",
    "severity_byState_Moderate=severity_byState[severity_byState.SeverityTypeName=='Moderate']\n",
    "severity_byState_Moderate=severity_byState_Moderate.drop('SeverityTypeName',axis=1)\n",
    "severity_byState_Moderate=severity_byState_Moderate.set_index('StateName')\n",
    "# minor accidents by state\n",
    "severity_byState_Minor=severity_byState[severity_byState.SeverityTypeName=='Minor']\n",
    "severity_byState_Minor=severity_byState_Minor.drop('SeverityTypeName',axis=1)\n",
    "severity_byState_Minor=severity_byState_Minor.set_index('StateName')\n",
    "# death causing accidents by state\n",
    "severity_byState_Death=severity_byState[severity_byState.SeverityTypeName=='Death']\n",
    "severity_byState_Death=severity_byState_Death.drop('SeverityTypeName',axis=1)\n",
    "severity_byState_Death=severity_byState_Death.set_index('StateName')\n",
    "# life threatening accidents by state\n",
    "severity_byState_Life_threatening=severity_byState[severity_byState.SeverityTypeName=='Life-threatening']\n",
    "severity_byState_Life_threatening=severity_byState_Life_threatening.drop('SeverityTypeName',axis=1)\n",
    "severity_byState_Life_threatening=severity_byState_Life_threatening.set_index('StateName')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.figure(1)\n",
    "plt.subplot(212)\n",
    "severity_byState_major.plot(kind='bar')\n",
    "plt.ylabel('number of major accidents')\n",
    "plt.title('Major Accidents by State')\n",
    "plt.figure(2)\n",
    "plt.subplot(211)\n",
    "severity_byState_Moderate.plot(kind='bar')\n",
    "plt.ylabel('number of moderate accidents')\n",
    "plt.title('Moderate Accidents by State')\n",
    "\n",
    "plt.figure(3)\n",
    "plt.subplot(222)\n",
    "severity_byState_Minor.plot(kind='bar')\n",
    "plt.ylabel('number of minor accidents')\n",
    "plt.title('Minor Accidents by State')\n",
    "plt.figure(4)\n",
    "plt.subplot(223)\n",
    "severity_byState_Death.plot(kind='bar')\n",
    "plt.ylabel('number of death accidents')\n",
    "plt.title('Death Accidents by State')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pennsylvania has the highest number of Major Accident types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "severity_byState_major.plot(kind='bar')\n",
    "plt.title('Major Accidents category by states')\n",
    "plt.ylabel('number of accidents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_df['SeverityTypeName'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_df['InjuryDescription'] .dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_df['InjuryDescription'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower the texts in the column\n",
    "general_df['AccidentDescription'] = general_df['AccidentDescription'].str.lower() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove if there is any space before and after the texts\n",
    "general_df['AccidentDescription'] =general_df['AccidentDescription'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_df['AccidentDescription'] =general_df['AccidentDescription'].replace('[^\\w\\s]',regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's remove stop words from tokenized words.\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# let's download stop words\n",
    "nltk.download('stopwords')\n",
    "# set up stop words to enlish stop words\n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to apply spilt or word_tokenize to a column , you should change it to string type using astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the stops words from Injury description column and assing it back to general_df\n",
    "\n",
    "general_df['AccidentDescription'] = general_df['AccidentDescription'].astype(str).apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - predict causes of loss one by one\n",
    "## - mute and unmute one at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insured vehicle rear end claimant vehicle\n",
    "general_df['losscause'] = np.where(general_df['LossCauseName']=='IV rear-end CV',1,0)\n",
    "# collision at intersection or others\n",
    "#general_df['losscause'] = np.where(general_df['LossCauseName']=='Collision in an intersection',1,0)\n",
    "# if the cause is 'Collision with motor vehicle'\n",
    "#general_df['losscause'] = np.where(general_df['LossCauseName']=='Collision with motor vehicle',1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map severity levels to numeric\n",
    "            \n",
    "##general_df['LossCauseName']=general_df.SeverityTypeName.map({'Death':0,'Life-threatening':1,'Major (hospitalization 3+ days or ICU)':2,'Minor':3,'Moderate':4})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and Y\n",
    "\n",
    "X=general_df['AccidentDescription']\n",
    "y=general_df['losscause']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.30,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#from sklearn.metrics import Score\n",
    "#Train and evaluate the model\n",
    "# fit CountVectorizer to X_train data\n",
    "vect = CountVectorizer().fit(X_train)\n",
    "# transform X_train data\n",
    "X_train_vectorized = vect.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes model\n",
    "clfrNB = MultinomialNB(alpha = 0.1)\n",
    "# fit the model on vectorized data\n",
    "clfrNB.fit(X_train_vectorized, y_train)\n",
    "# predict loss cause\n",
    "pred = clfrNB.predict(vect.transform(X_test))\n",
    "# calculate score\n",
    "score = metrics.accuracy_score(y_test,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actually IV-rear-end-cv, but predcited not(FN)=1335\n",
    "### Actually not IV-rear-end-cv, but predicted as iv-rear-end-cv(FP)=4485\n",
    "### Actually iv-rear-end-cv, but predicted as iv-rear-end-cv(TP)=7899"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proportion of correct prediction to total prediction of IV-rear-end-cv \n",
    "# (i.e how much of the those identified as positives were actually positives)\n",
    "TP=7899\n",
    "FP=4485\n",
    "precision=TP/(TP+FP)\n",
    "print('precision',round((precision),2))\n",
    "\n",
    "\n",
    "FN=1335\n",
    "TN=22479  \n",
    "# recall is how much iv-rear-end-cv was correctly predicted (i.e how much of positives were correctly identified as positives)\n",
    "recall=TP/(TP+FN)\n",
    "print('recall',round((recall),2))\n",
    "\n",
    "# accuracy is the diagonal part\n",
    "# cp=correct prediction\n",
    "#wp=wrong prediction\n",
    "#TotP=Total Predictions\n",
    "cp=(22479  + 7899)\n",
    "wp= (1335 +4485)\n",
    "TotP=cp+wp\n",
    "accuracy_score=cp/TotP\n",
    "print('accuracy_score',round((accuracy_score),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('accuracy',round((score),2))\n",
    "cm=confusion_matrix(y_test, pred)\n",
    "print(cm)\n",
    "\n",
    "auc = roc_auc_score(y_test, pred)\n",
    "print('AUC',round((auc),2))\n",
    "\n",
    "print(classification_report(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrix\n",
    "plot_confusion_matrix(y_test, pred, classes=[1,0],\n",
    "                      title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_df['InjuryDescription_tokens']=general_df['InjuryDescription'].astype(str).apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_df['InjuryDescription_tokens'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test['tweet'].apply(lambda x: [item for item in x if item not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_df['word_counts_InjuryDescr'] = [dict(Counter(doc)) for doc in general_df['InjuryDescription_tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_df['word_counts_InjuryDescr'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary = Dictionary(general_df['InjuryDescription_tokens'])\n",
    "#print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus = [dictionary.doc2bow(doc) for doc in general_df['InjuryDescription_tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Tfidf model from gensim.models.tfidfmodel\n",
    "\n",
    "#from gensim.models.tfidfmodel import TfidfModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give weights to  tokens infifth line in my documents \n",
    "#tfidf=TfidfModel(corpus)\n",
    "# calculate tfidf weights by passing corpus to tfidf\n",
    "#tfidf[corpus[4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cause of accidents \n",
    "cause_df=general_df[['ClaimID_RGEN','LossCauseName']].groupby('LossCauseName').count()\n",
    "# rename columns and then plot\n",
    "cause_df.rename(columns={'ClaimID_RGEN':'count'},inplace=True)\n",
    "print(cause_df)\n",
    "cause_df.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cause_df['cause_share']=(cause_df['count'])/(len(general_df))\n",
    "cause_df=cause_df.drop('count',axis=1).sort_values('cause_share',ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cause_df=cause_df.head(10)\n",
    "cause_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cause_df.plot(kind='bar')\n",
    "plt.ylabel('Percentage')\n",
    "plt.xlabel('')\n",
    "plt.title(\"Percentage Share of Accident Causes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(general_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_df['LossCauseName']=general_df.LossCauseName.map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# who was at fault\n",
    "fualts=general_df[['ClaimID_RGEN','FaultRatingName']].groupby('FaultRatingName').count()\n",
    "#.rename(columns=('ClaimID_RGEN':'count',inplace=True)\n",
    "fualts.rename(columns={'ClaimID_RGEN':'count'},inplace=True)\n",
    "fualts.plot(kind='bar')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
